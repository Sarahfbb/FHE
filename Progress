1.Single-multi
~S_Training.py(No normal layer)
Final Accuracies - Train: 99.73%, Val: 85.23%, Test: 84.37%
Best Validation Accuracy: 85.71%
Save as "fhe_friendly_mlp_model.pth"
~S count flops.py"
VGG Feature Extractor FLOPs: 626,895,360
MLP Classifier FLOPs: 13,063,680
Total FLOPs (VGG + MLP): 639,959,040

~S_qat_training under then 2S_qat_training(With  Batch Normal layer)
Train Loss=0.0458, Train Acc=99.08%, Val Acc=84.18%]
Final Test Accuracy: 83.63%
Plots saved as '2combined_plot.png'
Save as "fhe_friendly_qat_mlp_model.pth"

~S_fhe_compilation_simulation.py
FHE Simulation Results:
Rounding Threshold   FHE Acc (%)     Torch Acc (%)   FHE Time (s)    Torch Time (s) 
--------------------------------------------------------------------------------
8                    82.91           83.63           87.03           12.80
7                    82.89           83.63           79.26           12.80
6                    82.71           83.63           83.44           12.80
5                    82.49           83.63           81.69           12.80
3                    66.03           83.63           79.46           12.80

Best rounding threshold: 8
Maximum bit-width in the circuit: 14

Average FHE simulation accuracy: 79.41%
Average FHE simulation time: 82.17 seconds
Total execution time: 528.96 seconds

~S_fhe_actual_execution_benchmark.py
Actual FHE Performance (1 sample):
Inference Time: 17.6834 seconds
Memory Used: 141545.57 KB
Prediction Correct: True

Detailed Results:
Number of samples: 1
Input dimension: 25088
Number of classes: 10
Bit Width: 6
P Error: 0.01
Inference Time: 17.6834 seconds
Memory Used: 141545.57 KB
Prediction Correct: True

2 Multiple binary
~2 M Training + Evolutionary
Saved as 'fhe_friendly_ensemble_class_{class_id}_mlp_{i}.pt'
And the best ensemble saved as 'best_ensembles.pt'
Best ensemble validation accuracy: 0.8544
Best ensemble test accuracy: 0.8473
Best ensemble composition:
Class 0: Model 7
Class 1: Model 4
Class 2: Model 7
Class 3: Model 4
Class 4: Model 4
Class 5: Model 7
Class 6: Model 9
Class 7: Model 5
Class 8: Model 1
Class 9: Model 3

~M count flops
VGG Feature Extractor FLOPs: 626,895,360
MLP Classifier for class 0 (Model 7) FLOPs: 13,061,376
MLP Classifier for class 1 (Model 4) FLOPs: 13,061,376
MLP Classifier for class 2 (Model 7) FLOPs: 13,061,376
MLP Classifier for class 3 (Model 4) FLOPs: 13,061,376
MLP Classifier for class 4 (Model 4) FLOPs: 13,061,376
MLP Classifier for class 5 (Model 7) FLOPs: 13,061,376
MLP Classifier for class 6 (Model 9) FLOPs: 13,061,376
MLP Classifier for class 7 (Model 5) FLOPs: 13,061,376
MLP Classifier for class 8 (Model 1) FLOPs: 13,061,376
MLP Classifier for class 9 (Model 3) FLOPs: 13,061,376

Total MLP Classifiers FLOPs: 130,613,760
Total FLOPs (VGG + Ensemble of 10 MLPs): 757,509,120

~M QAT
save as"Mfine_tuned_quantized_mlp_model.pth" and 'Mfinal_brevitas_quantized_mlp_model.pth'
Final ensemble accuracies - Val: 99.70%, Test: 83.80%

