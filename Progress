1.Single-multi
~2 S Training.py
Final Accuracies - Train: 99.75%, Val: 84.91%, Test: 83.82%
Saved as 'fhe_friendly_mlp_model.pth'

~S count flops.py"
VGG Feature Extractor FLOPs: 626,895,360
MLP Classifier FLOPs: 13,063,680
Total FLOPs (VGG + MLP): 639,959,040

~S QAT
save as"Sfine_tuned_quantized_mlp_model.pth" and 'Sfinal_brevitas_quantized_mlp_model.pth'

Final Accuracies - Val: 99.92%, Test: 83.25%
2 Multiple binary
~2 M Training + Evolutionary
Saved as 'fhe_friendly_ensemble_class_{class_id}_mlp_{i}.pt'
And the best ensemble saved as 'best_ensembles.pt'
Best ensemble validation accuracy: 0.8544
Best ensemble test accuracy: 0.8473
Best ensemble composition:
Class 0: Model 7
Class 1: Model 4
Class 2: Model 7
Class 3: Model 4
Class 4: Model 4
Class 5: Model 7
Class 6: Model 9
Class 7: Model 5
Class 8: Model 1
Class 9: Model 3

~M count flops
VGG Feature Extractor FLOPs: 626,895,360
MLP Classifier for class 0 (Model 7) FLOPs: 13,061,376
MLP Classifier for class 1 (Model 4) FLOPs: 13,061,376
MLP Classifier for class 2 (Model 7) FLOPs: 13,061,376
MLP Classifier for class 3 (Model 4) FLOPs: 13,061,376
MLP Classifier for class 4 (Model 4) FLOPs: 13,061,376
MLP Classifier for class 5 (Model 7) FLOPs: 13,061,376
MLP Classifier for class 6 (Model 9) FLOPs: 13,061,376
MLP Classifier for class 7 (Model 5) FLOPs: 13,061,376
MLP Classifier for class 8 (Model 1) FLOPs: 13,061,376
MLP Classifier for class 9 (Model 3) FLOPs: 13,061,376

Total MLP Classifiers FLOPs: 130,613,760
Total FLOPs (VGG + Ensemble of 10 MLPs): 757,509,120

~M QAT
save as"Mfine_tuned_quantized_mlp_model.pth" and 'Mfinal_brevitas_quantized_mlp_model.pth'
