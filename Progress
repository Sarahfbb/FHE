1.Single-multi
~S_Training.py(No normal layer)
Final Accuracies - Train: 99.73%, Val: 85.23%, Test: 84.37%
Best Validation Accuracy: 85.71%
Save as "fhe_friendly_mlp_model.pth"
~S count flops.py"
VGG Feature Extractor FLOPs: 626,895,360
MLP Classifier FLOPs: 13,063,680
Total FLOPs (VGG + MLP): 639,959,040

~S_qat_training under then 2S_qat_training(With  Batch Normal layer)
Train Loss=0.0458, Train Acc=99.08%, Val Acc=84.18%]
Final Test Accuracy: 83.63%
Plots saved as '2combined_plot.png'
Save as "fhe_friendly_qat_mlp_model.pth"

~check.py
check the compatibility
2 Multiple binary
~2 M Training + Evolutionary
Saved as 'fhe_friendly_ensemble_class_{class_id}_mlp_{i}.pt'
And the best ensemble saved as 'best_ensembles.pt'
Best ensemble validation accuracy: 0.8544
Best ensemble test accuracy: 0.8473
Best ensemble composition:
Class 0: Model 7
Class 1: Model 4
Class 2: Model 7
Class 3: Model 4
Class 4: Model 4
Class 5: Model 7
Class 6: Model 9
Class 7: Model 5
Class 8: Model 1
Class 9: Model 3

~M count flops
VGG Feature Extractor FLOPs: 626,895,360
MLP Classifier for class 0 (Model 7) FLOPs: 13,061,376
MLP Classifier for class 1 (Model 4) FLOPs: 13,061,376
MLP Classifier for class 2 (Model 7) FLOPs: 13,061,376
MLP Classifier for class 3 (Model 4) FLOPs: 13,061,376
MLP Classifier for class 4 (Model 4) FLOPs: 13,061,376
MLP Classifier for class 5 (Model 7) FLOPs: 13,061,376
MLP Classifier for class 6 (Model 9) FLOPs: 13,061,376
MLP Classifier for class 7 (Model 5) FLOPs: 13,061,376
MLP Classifier for class 8 (Model 1) FLOPs: 13,061,376
MLP Classifier for class 9 (Model 3) FLOPs: 13,061,376

Total MLP Classifiers FLOPs: 130,613,760
Total FLOPs (VGG + Ensemble of 10 MLPs): 757,509,120

~M QAT
save as"Mfine_tuned_quantized_mlp_model.pth" and 'Mfinal_brevitas_quantized_mlp_model.pth'
Final ensemble accuracies - Val: 99.70%, Test: 83.80%

