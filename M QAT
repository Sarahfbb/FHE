import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
import torchvision.models as models
from brevitas.nn import QuantLinear
from brevitas.quant import Int8Bias

# VGG Feature Extractor
class VGGFeatureExtractor(nn.Module):
    def __init__(self, vgg_model):
        super(VGGFeatureExtractor, self).__init__()
        self.features = vgg_model.features
        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))

    def forward(self, x):
        x = self.features(x)
        x = self.avgpool(x)
        return torch.flatten(x, 1)

# Original MLP Classifier
class MLPClassifier(nn.Module):
    def __init__(self, input_dim, num_classes):
        super(MLPClassifier, self).__init__()
        self.fc1 = nn.Linear(input_dim, 256)
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, num_classes)
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = torch.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)
        return x

# Quantized MLP Classifier
class QuantMLPClassifier(nn.Module):
    def __init__(self, input_dim, num_classes):
        super(QuantMLPClassifier, self).__init__()
        self.fc1 = QuantLinear(input_dim, 256, bias=True, weight_bit_width=8, bias_quant=Int8Bias)
        self.fc2 = QuantLinear(256, 128, bias=True, weight_bit_width=8, bias_quant=Int8Bias)
        self.fc3 = QuantLinear(128, num_classes, bias=True, weight_bit_width=8, bias_quant=Int8Bias)
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = torch.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)
        return x

# Combined model
class QuantVGGMLP(nn.Module):
    def __init__(self, vgg_model, mlp_model):
        super(QuantVGGMLP, self).__init__()
        self.feature_extractor = VGGFeatureExtractor(vgg_model)
        self.classifier = mlp_model

    def forward(self, x):
        features = self.feature_extractor(x)
        return self.classifier(features)

def convert_to_quantized(pretrained_model, quantized_model):
    pretrained_dict = pretrained_model.state_dict()
    quantized_dict = quantized_model.state_dict()
    
    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in quantized_dict and 'weight' in k}
    quantized_dict.update(pretrained_dict)
    
    quantized_model.load_state_dict(quantized_dict)

def fine_tune_and_qat(model, train_loader, val_loader, epochs, lr, device):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    
    best_val_loss = float('inf')
    best_model_state = None
    
    for epoch in range(epochs):
        model.train()
        train_loss = 0.0
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                val_loss += loss.item()
        
        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}')
        
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model_state = model.state_dict()
    
    # Load the best model state
    model.load_state_dict(best_model_state)
    
    # Save the best model
    torch.save(best_model_state, 'best_quant_model.pt')
    
    return model

def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # Load CIFAR-10 dataset
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])
    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
    valset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
    
    train_loader = DataLoader(trainset, batch_size=64, shuffle=True)
    val_loader = DataLoader(valset, batch_size=64, shuffle=False)

    # Load pre-trained VGG16
    vgg16 = models.vgg16(pretrained=True)
    
    # Create the original model
    original_mlp = MLPClassifier(512 * 7 * 7, 10)
    original_mlp.load_state_dict(torch.load('mlp_model.pth'))

    # Create and initialize the quantized model
    quant_mlp = QuantMLPClassifier(512 * 7 * 7, 10).to(device)
    convert_to_quantized(original_mlp, quant_mlp)

    quant_vgg_mlp = QuantVGGMLP(vgg16, quant_mlp).to(device)

    # Fine-tune and apply QAT
    fine_tuned_quant_model = fine_tune_and_qat(quant_vgg_mlp, train_loader, val_loader, epochs=10, lr=0.0001, device=device)
    
    print("Fine-tuned quantized model saved as 'best_quant_model.pt'.")

if __name__ == '__main__':
    main()
